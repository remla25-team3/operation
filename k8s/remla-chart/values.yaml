# operation/k8s/remla-chart/values.yaml

replicaCount: 1

appFrontend:
  name: app-frontend
  image:
    repository: ghcr.io/remla25-team3/app-frontend
    tag: v1.3.2
    pullPolicy: IfNotPresent
  servicePort: 80
  containerPort: 80
  replicas: 1

appService:
  name: app-service
  image:
    repository: ghcr.io/remla25-team3/app-service
    tag: v1.4.1
    pullPolicy: IfNotPresent
  servicePort: 3000
  containerPort: 3000
  replicas: 1
  configData:
    IS_PRODUCTION: "false"
    LOG_LEVEL: "info"
  env:
    - name: PORT
      value: "3000"
    - name: MODEL_SERVICE_URL
      value: "{{ .Values.modelService.name }}"
    - name: MODEL_SERVICE_PORT
      value: "{{ .Values.modelService.servicePort }}"

modelService:
  name: model-service
  image:
    repository: ghcr.io/remla25-team3/model-service
    tag: v0.4.1
    pullPolicy: IfNotPresent
  servicePort: 5000
  containerPort: 5000
  replicas: 1
  env:
    - name: PORT
      value: "{{ .Values.modelService.servicePort }}"
    - name: RESOURCE_URL
      value: "https://github.com/remla25-team3/model-training/releases/download/"
    - name: MODEL
      value: "sentiment_model.pkl"
    - name: CV
      value: "bow_sentiment_model.pkl"
    - name: MODEL_VERSION
      value: "v0.2.3"
  persistence:
    enabled: true
    size: 1Gi

serviceMonitor:
  enabled: true
  path: /metrics
  interval: 30s
  prometheusReleaseLabel: "prometheus-stack"

alerting:
  rulesEnabled: true
  prometheusRules:
    - name: request-rate
      rules:
      - alert: HighRequestRate
        expr: sum(rate(http_requests_total[1m])) > 15
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High request rate detected!"
          description: "The application is receiving more than 15 requests per minute for 2 minutes."
    - name: error-rate
      rules:
      - alert: ErrorRateHigh
        expr: sum(rate(http_requests_errors_total[5m])) / sum(rate(http_requests_total[5m])) * 100 > 5
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected!"
          description: "Error rate is above 5% for the last 5 minutes."
    - name: resource-usage
      rules:
      - alert: HighMemoryUsage
        expr: container_memory_usage_bytes{container!=""} / container_spec_memory_limit_bytes{container!=""} * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage detected!"
          description: "Container {{`{{ $labels.container }}`}} is using more than 85% of its memory limit."
      - alert: HighCPUUsage
        expr: sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (container) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage detected!"
          description: "Container {{`{{ $labels.container }}`}} is using more than 80% of CPU."
    - name: availability
      rules:
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service is down!"
          description: "Service {{`{{ $labels.service }}`}} has been down for more than 1 minute."
      - alert: HighLatency
        expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency detected!"
          description: "95th percentile of request latency is above 1 second."

  alertmanager:
    enabled: true
    smtp:
      smtp_require_tls: true
      passwordSecret:
        name: "{{ .Release.Name }}-alert-manager-secret"
        key: smtp_auth_password
      usernameSecret:
        name: "{{ .Release.Name }}-alert-manager-secret"
        key: smtp_auth_username

    recipients:
      # I'm really desperate for emails
      critical: "dkrylov@tudelft.nl"
      warning: "dkrylov@tudelft.nl"
      info: "dkrylov@tudelft.nl"
      default: "dkrylov@tudelft.nl"

kube-prometheus-stack:
  alertmanager:
    enabled: true
    configMapOverrideName: "{{ .Release.Name }}-alert-manager-config"

  prometheus:
    prometheusSpec:
      maximumStartupDurationSeconds: 120
      alerting:
        alertmanagers:
        - namespace: monitoring
          name: "remla-app-kube-prometheus-stack-alertmanager"
          port: web
      serviceMonitorSelectorNilUsesHelmValues: false
      serviceMonitorNamespaceSelector: {}

grafana:
  enabled: true
  ingress:
    enabled: true
    hostname: grafana.local
    serviceName: grafana
  servicePort: 80
  sidecar:
    dashboards:
      enabled: true
      searchNamespace: monitoring

apiIngress:
  hostname: api.local

appIngress:
  hostname: remla.local