- name: Finalization
  hosts: all
  become: true
  vars_files:
    - vars/all.yml
  tasks:

  #step 20
  - name: Ensure resources/ dir exists
    delegate_to: localhost
    connection: local
    become: false
    run_once: true
    ansible.builtin.file:
      path: "{{ playbook_dir }}/resources"
      state: directory
      mode: '0755'

  - name: Check for local MetalLB manifest
    delegate_to: localhost
    connection: local      # run it directly, no SSH
    become: false          # don't try sudo on localhost
    run_once: true
    ansible.builtin.stat:
      path: "{{ playbook_dir }}/resources/metallb-native.yaml"
    register: metallb_manifest

  - name: Download MetalLB manifest if not found locally (step 20)
    delegate_to: localhost
    connection: local
    become: false
    run_once: true
    get_url:
      url: https://raw.githubusercontent.com/metallb/metallb/v0.14.9/config/manifests/metallb-native.yaml
      dest: "{{ playbook_dir }}/resources/metallb-native.yaml"
      mode: '0644'
    when: not metallb_manifest.stat.exists

  - name: Copy MetalLB manifest to controller
    copy:
      src: "{{ playbook_dir }}/resources/metallb-native.yaml"
      dest: /tmp/metallb-native.yaml
      mode: '0644'

  - name: Deploy MetalLB using official manifest (step 20)
    kubernetes.core.k8s:
      state: present
      src: /tmp/metallb-native.yaml
      kubeconfig: "{{ kubeconfig }}"

  - name: Remove NoSchedule taints from controller node (step 20)
    shell: |
      kubectl taint nodes ctrl node-role.kubernetes.io/control-plane:NoSchedule- || true
      kubectl taint nodes ctrl node-role.kubernetes.io/master:NoSchedule- || true
    changed_when: false

  - name: Wait for MetalLB controller pod to become ready (step 20)
    shell: |
      kubectl wait -n metallb-system -l app=metallb,component=controller --for=condition=ready pod --timeout=60s --kubeconfig={{ kubeconfig }}
    changed_when: false

  - name: Create IPAddressPool manifest for MetalLB (step 20)
    copy:
      dest: /tmp/metallb-pool.yaml
      mode: '0644'
      content: |
        apiVersion: metallb.io/v1beta1
        kind: IPAddressPool
        metadata:
          name: remla-pool
          namespace: metallb-system
        spec:
          addresses:
            - {{ ip_range }}

  - name: Apply IPAddressPool manifest (step 20)
    kubernetes.core.k8s:
      state: present
      src: /tmp/metallb-pool.yaml
      kubeconfig: "{{ kubeconfig }}"

  - name: Create L2Advertisement manifest for MetalLB (step 20)
    copy:
      dest: /tmp/metallb-l2adv.yaml
      mode: '0644'
      content: |
        apiVersion: metallb.io/v1beta1
        kind: L2Advertisement
        metadata:
          name: remla-l2
          namespace: metallb-system
        spec:
          ipAddressPools:
            - remla-pool

  - name: Apply L2Advertisement manifest (step 20)
    kubernetes.core.k8s:
      state: present
      src: /tmp/metallb-l2adv.yaml
      kubeconfig: "{{ kubeconfig }}"

  #step 21
  - name: Add ingress-nginx Helm repository (step 21)
    kubernetes.core.helm_repository:
      name: ingress-nginx
      repo_url: https://kubernetes.github.io/ingress-nginx
      kubeconfig: "{{ kubeconfig }}"

  - name: Install ingress-nginx with static LoadBalancer IP (step 21)
    kubernetes.core.helm:
      name: ingress-nginx
      chart_ref: ingress-nginx/ingress-nginx
      chart_version: "4.11.0"
      release_namespace: ingress-nginx
      create_namespace: true
      kubeconfig: "{{ kubeconfig }}"
      values:
        controller:
          service:
            type: LoadBalancer
            loadBalancerIP: "{{ ingress_ip }}"
          ingressClassResource:
            name: nginx
          ingressClass: nginx
  
  - name: Wait for NGINX Ingress controller pod to become ready (step 21)
    shell: |
      kubectl --kubeconfig={{ kubeconfig }} wait -n ingress-nginx \
      --for=condition=ready pod \
      -l app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/component=controller \
      --timeout=120s
    changed_when: false

  #step 22
  - name: Add Kubernetes Dashboard Helm repo (step 22)
    kubernetes.core.helm_repository:
      name: kubernetes-dashboard
      repo_url: https://kubernetes.github.io/dashboard
      kubeconfig: "{{ kubeconfig }}"

  - name: Install Kubernetes Dashboard via Helm (step 22)
    kubernetes.core.helm:
      name: "{{ dashboard_release }}"
      chart_ref: kubernetes-dashboard/kubernetes-dashboard
      release_namespace: "{{ dashboard_namespace }}"
      create_namespace: true
      kubeconfig: "{{ kubeconfig }}"
      force: true

  - name: Create admin-user ServiceAccount and ClusterRoleBinding (step 22)
    copy:
      dest: /tmp/dashboard-adminuser.yaml
      mode: '0644'
      content: |
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: admin-user
          namespace: {{ dashboard_namespace }}
        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          name: admin-user
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: cluster-admin
        subjects:
          - kind: ServiceAccount
            name: admin-user
            namespace: {{ dashboard_namespace }}

  - name: Apply admin-user access (step 22)
    kubernetes.core.k8s:
      state: present
      src: /tmp/dashboard-adminuser.yaml
      kubeconfig: "{{ kubeconfig }}"

  - name: Create Ingress for Dashboard (step 22)
    copy:
      dest: /tmp/dashboard-ingress.yaml
      mode: '0644'
      content: |
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        metadata:
          name: kubernetes-dashboard
          namespace: {{ dashboard_namespace }}
          annotations:
            nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
            nginx.ingress.kubernetes.io/ssl-redirect: "true"
            nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
        spec:
          ingressClassName: nginx
          tls:
            - hosts:
                - {{ ingress_host }}
          rules:
            - host: {{ ingress_host }}
              http:
                paths:
                  - path: /
                    pathType: Prefix
                    backend:
                      service:
                        name: kubernetes-dashboard-kong-proxy
                        port:
                          number: 443

  - name: Apply Dashboard Ingress (step 22)
    kubernetes.core.k8s:
      state: present
      src: /tmp/dashboard-ingress.yaml
      kubeconfig: "{{ kubeconfig }}"

  - name: Wait for Dashboard proxy service to become available (step 22)
    shell: |
      kubectl wait --namespace={{ dashboard_namespace }} \
        --for=condition=ready pod \
        -l app.kubernetes.io/instance={{ dashboard_release }} \
        --timeout=300s --kubeconfig={{ kubeconfig }}
    changed_when: false

  - name: Display dashboard instructions (step 22)
    debug:
      msg:
        - "Kubernetes Dashboard deployed and accessible via Ingress."
        - ""
        - "To access it, follow these steps:"
        - ""
        - "1. Open your terminal on the host machine and run:"
        - "  sudo nano /etc/hosts"
        - ""
        - "2. Add the following line to the bottom of the file:"
        - "  {{ ingress_ip }}   dashboard.local"
        - ""
        - "3. Save and exit nano:"
        - "  Press Ctrl + O (to write the file), then Enter, then Ctrl + X (to exit)"
        - ""
        - "4. Open your browser and visit:"
        - "  https://dashboard.local"
        - ""
        - "5. To log in, run the following command on the controller VM to generate a token:"
        - "  vagrant ssh ctrl"
        - "  kubectl -n {{ dashboard_namespace }} create token admin-user"
        - ""
        - "6. Paste the token into the Dashboard login screen."

  - name: Remove NoSchedule taints from all nodes
    shell: |
      kubectl taint nodes --all node-role.kubernetes.io/control-plane:NoSchedule- || true
      kubectl taint nodes --all node-role.kubernetes.io/master:NoSchedule- || true
    changed_when: false

  # -----------------STEP 23: Install Istio-----------------

  - name: Detect system architecture (step 23)
    ansible.builtin.command: uname -m
    register: system_arch
    changed_when: false

  - name: Set Istio architecture variable (step 23)
    ansible.builtin.set_fact:
      istio_arch: >-
        {%- if system_arch.stdout in ['x86_64', 'amd64'] -%}
        amd64
        {%- elif system_arch.stdout in ['aarch64', 'arm64'] -%}
        arm64
        {%- else -%}
        {{ system_arch.stdout }}
        {%- endif -%}

  - name: Display detected architecture (step 23)
    ansible.builtin.debug:
      msg: "Detected architecture: {{ system_arch.stdout }} -> Using Istio arch: {{ istio_arch }}"

  - name: Add Istio (step 23)
    ansible.builtin.unarchive:
      src: https://github.com/istio/istio/releases/download/1.25.2/istio-1.25.2-linux-{{ istio_arch }}.tar.gz
      dest: /usr/local/bin
      remote_src: yes
      mode: 'u=rX'

  - name: Set user permissions for istioctl command (step 23)
    ansible.builtin.file:
      path: /usr/local/bin/istio-1.25.2
      state: directory
      recurse: yes
      owner: vagrant
      group: vagrant
      mode: 'u=rX'

  - name: Get contents of PATH (step 23)
    command: cat /etc/environment
    register: path_contents

  - name: Add PATH line if it did not exist in /etc/environment (step 23)
    ansible.builtin.lineinfile:
      path: /etc/environment
      line: 'PATH=""'
      insertafter: EOF
      state: present
      create: true
    when: not path_contents|regex_search('\s*PATH=.*')

  - name: Add istioctl to PATH (step 23)
    ansible.builtin.lineinfile:
      path: /etc/environment
      regexp: "^PATH=(\"?)([^\"]*)(\"?)$"
      line: 'PATH=\1\2:/usr/local/bin/istio-1.25.2/bin\3'
      backrefs: yes
    when: not path_contents|regex_search('istio-1.25.2')

  - name: Check if istio namespace already existed (step 23)
    kubernetes.core.k8s_info:
      kubeconfig: "{{ kubeconfig }}"
      kind: Namespace
      name: istio-system
    register: istio_system_ns_info
    ignore_errors: true

  # - name: Set user permissions for kubeconfig (step 23)
  #   ansible.builtin.file:
  #     path: "{{ kubeconfig }}"
  #     state: file
  #     mode: 'a=xwr'

  - name: Run Istio installation on controller node (step 23)
    become: false
    shell: '/usr/local/bin/istio-1.25.2/bin/istioctl install -y --set profile=minimal --set components.pilot.k8s.resources.requests.memory=512Mi'
    changed_when: false
    when: istio_system_ns_info.resources | length == 0

#  - name: Wait for Istio pods to run (step 23)
#    shell: |
#      kubectl wait -n istio-system --for=condition=available deployment/istiod --timeout=60s --kubeconfig={{ kubeconfig }}
#    changed_when: false

  # - name: Copy application k8s manifests
  #   copy:
  #     src: "{{ playbook_dir }}/../kubernetes/"
  #     dest: /tmp/kubernetes
  #     mode: '0644'

  # - name: Retrieve manifest file names as list
  #   ansible.builtin.find:
  #     paths: /tmp/kubernetes
  #     patterns: '*.yml,*.yaml'
  #   register: k8s_manifest_files

  # - name: Set filename variable
  #   set_fact:
  #     k8s_manifest_file_list: "{{ k8s_manifest_files.files | map(attribute='path') | list }}"

  # - name: Apply application k8s manifests
  #   kubernetes.core.k8s:
  #     state: present
  #     src: "{{ item }}"
  #     namespace: default
  #     kubeconfig: "{{ kubeconfig }}"
  #     wait: true
  #     timeout: "300s"
  #     values:
  #       pilot:
  #         resources:
  #           # shrink requests so istiod can schedule on low‐RAM nodes (too heave otherwise, might need to be changed in the future)
  #           requests:
  #             cpu: "100m"
  #             memory: "128Mi"
  #           limits:
  #             cpu: "200m"
  #             memory: "256Mi"
  #   loop: "{{ k8s_manifest_file_list }}"

  - name: Wait for Istio control plane (istiod) to be ready
    shell: |
      kubectl wait --for=condition=ready pod -l app=istiod -n istio-system --timeout=300s --kubeconfig={{ kubeconfig }}
    changed_when: false

  - name: Add Istio Helm repository
    kubernetes.core.helm_repository:
      name: istio
      repo_url: "https://istio-release.storage.googleapis.com/charts"
      state: present

  # - name: Uninstall existing Istio ingress gateway (if any)
  #   kubernetes.core.helm:
  #     release_name: istio-ingress
  #     release_namespace: istio-system
  #     kubeconfig: "{{ kubeconfig }}"
  #     state: absent
  #     wait: true
  #   ignore_errors: true

  - name: Write istio-ingress values to file
    copy:
      dest: /tmp/istio-ingress-values.yaml
      mode: '0644'
      content: |
        service:
          type: LoadBalancer
          loadBalancerIP: "{{ istio_ip }}"
          ports:
            - name: status-port
              port: 15021
              targetPort: 15021
            - name: http
              port: 80
              targetPort: 8080
            - name: https
              port: 443
              targetPort: 8443


  - name: Deploy Istio ingress gateway with fixed IP (step 23)
    kubernetes.core.helm:
      release_name: istio-ingress
      chart_ref: istio/gateway
      chart_version: "1.25.2"
      release_namespace: istio-system
      kubeconfig: "{{ kubeconfig }}"
      wait: true
      timeout: "300s"
      force: true
      values_files:
        - /tmp/istio-ingress-values.yaml

  - name: Wait for Istio ingress gateway to be ready (step 23)
    shell: |
      kubectl wait --for=jsonpath='{.status.containerStatuses[0].ready}'=true pod -l app.kubernetes.io/instance=istio-ingress -n istio-system --timeout=300s --kubeconfig={{ kubeconfig }}
    changed_when: false

  - name: Istio deployment summary (step 23)
    debug:
      msg:
        - "Istio control plane and ingress gateway have been successfully installed."
        - "The gateway is exposed on the fixed IP: {{ istio_ip }}"
        - "Your cluster is now prepared for Istio-based deployments."
  
  # - name: Create monitoring namespace
  #   kubernetes.core.k8s:
  #     state: present
  #     definition:
  #       apiVersion: v1
  #       kind: Namespace
  #       metadata:
  #         name: "{{ monitoring_namespace }}"
  #     kubeconfig: "{{ kubeconfig }}"

  - name: Label monitoring namespace for Istio injection
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: v1
        kind: Namespace
        metadata:
          name: default
          labels:
            istio-injection: enabled
      kubeconfig: "{{ kubeconfig }}"

  - name: Copy the entire remla-chart directory to the controller node
    copy:
      src: "{{ playbook_dir }}/../k8s/remla-chart"
      dest: /home/vagrant
      owner: vagrant
      group: vagrant
      mode: '0755'
  
  - name: Install the REMLA application stack via Helm chart
    kubernetes.core.helm:
      name: remla-app
      chart_ref: /home/vagrant/remla-chart
      release_namespace: default
      kubeconfig: "{{ kubeconfig }}"
      state: present
      update_repo_cache: yes

  #A5

  # - name: Copy gateway setup configuration (A5, Traffic Management)
  #   copy:
  #     src: "{{ playbook_dir }}/resources/gateway-setup.yml"
  #     dest: /tmp/gateway-setup.yml
  #     mode: '0644'

  # - name: Start VirtualService (A5, Traffic Management)
  #   kubernetes.core.k8s:
  #     state: present
  #     src: /tmp/gateway-setup.yml
  #     kubeconfig: "{{ kubeconfig }}"
 
