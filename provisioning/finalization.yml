- hosts: all
  become: true
  vars:
    kubeconfig: /etc/kubernetes/admin.conf
    ip_range: "192.168.56.90-192.168.56.99"
    ingress_ip: "192.168.56.91"  # must be within the MetalLB pool
    istio_ip : "192.168.56.92"  # must be within the MetalLB pool
    dashboard_namespace: kubernetes-dashboard
    dashboard_release: kubernetes-dashboard
    ingress_host: dashboard.local
    monitoring_namespace: monitoring
    prometheus_release_name: prom-monitoring
    app_service_name: app-service
  tasks:

  #step 20
  - name: Ensure resources/ dir exists
    delegate_to: localhost
    connection: local
    become: false
    run_once: true
    ansible.builtin.file:
      path: "{{ playbook_dir }}/resources"
      state: directory
      mode: '0755'

  - name: Check for local MetalLB manifest
    delegate_to: localhost
    connection: local      # run it directly, no SSH
    become: false          # don't try sudo on localhost
    run_once: true
    ansible.builtin.stat:
      path: "{{ playbook_dir }}/resources/metallb-native.yaml"
    register: metallb_manifest

  - name: Download MetalLB manifest if not found locally (step 20)
    delegate_to: localhost
    connection: local
    become: false
    run_once: true
    get_url:
      url: https://raw.githubusercontent.com/metallb/metallb/v0.14.9/config/manifests/metallb-native.yaml
      dest: "{{ playbook_dir }}/resources/metallb-native.yaml"
      mode: '0644'
    when: not metallb_manifest.stat.exists

  - name: Copy MetalLB manifest to controller
    copy:
      src: "{{ playbook_dir }}/resources/metallb-native.yaml"
      dest: /tmp/metallb-native.yaml
      mode: '0644'

  - name: Deploy MetalLB using official manifest (step 20)
    kubernetes.core.k8s:
      state: present
      src: /tmp/metallb-native.yaml
      kubeconfig: "{{ kubeconfig }}"

  - name: Remove NoSchedule taints from controller node (step 20)
    shell: |
      kubectl taint nodes ctrl node-role.kubernetes.io/control-plane:NoSchedule- || true
      kubectl taint nodes ctrl node-role.kubernetes.io/master:NoSchedule- || true
    changed_when: false

  - name: Wait for MetalLB controller pod to become ready (step 20)
    shell: |
      kubectl wait -n metallb-system -l app=metallb,component=controller --for=condition=ready pod --timeout=60s --kubeconfig={{ kubeconfig }}
    changed_when: false

  - name: Create IPAddressPool manifest for MetalLB (step 20)
    copy:
      dest: /tmp/metallb-pool.yaml
      mode: '0644'
      content: |
        apiVersion: metallb.io/v1beta1
        kind: IPAddressPool
        metadata:
          name: remla-pool
          namespace: metallb-system
        spec:
          addresses:
            - {{ ip_range }}

  - name: Apply IPAddressPool manifest (step 20)
    kubernetes.core.k8s:
      state: present
      src: /tmp/metallb-pool.yaml
      kubeconfig: "{{ kubeconfig }}"

  - name: Create L2Advertisement manifest for MetalLB (step 20)
    copy:
      dest: /tmp/metallb-l2adv.yaml
      mode: '0644'
      content: |
        apiVersion: metallb.io/v1beta1
        kind: L2Advertisement
        metadata:
          name: remla-l2
          namespace: metallb-system
        spec:
          ipAddressPools:
            - remla-pool

  - name: Apply L2Advertisement manifest (step 20)
    kubernetes.core.k8s:
      state: present
      src: /tmp/metallb-l2adv.yaml
      kubeconfig: "{{ kubeconfig }}"

  #step 21
  - name: Add ingress-nginx Helm repository (step 21)
    kubernetes.core.helm_repository:
      name: ingress-nginx
      repo_url: https://kubernetes.github.io/ingress-nginx
      kubeconfig: "{{ kubeconfig }}"

  - name: Install ingress-nginx with static LoadBalancer IP (step 21)
    kubernetes.core.helm:
      name: ingress-nginx
      chart_ref: ingress-nginx/ingress-nginx
      release_namespace: ingress-nginx
      create_namespace: true
      kubeconfig: "{{ kubeconfig }}"
      values:
        controller:
          service:
            type: LoadBalancer
            loadBalancerIP: "{{ ingress_ip }}"
          ingressClassResource:
            name: nginx
          ingressClass: nginx
  
  - name: Wait for NGINX Ingress controller pod to become ready (step 21)
    shell: |
      kubectl --kubeconfig={{ kubeconfig }} wait -n ingress-nginx \
      --for=condition=ready pod \
      -l app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/component=controller \
      --timeout=120s
    changed_when: false

  #step 22
  - name: Add Kubernetes Dashboard Helm repo (step 22)
    kubernetes.core.helm_repository:
      name: kubernetes-dashboard
      repo_url: https://kubernetes.github.io/dashboard
      kubeconfig: "{{ kubeconfig }}"

  - name: Install Kubernetes Dashboard via Helm (step 22)
    kubernetes.core.helm:
      name: "{{ dashboard_release }}"
      chart_ref: kubernetes-dashboard/kubernetes-dashboard
      release_namespace: "{{ dashboard_namespace }}"
      create_namespace: true
      kubeconfig: "{{ kubeconfig }}"

  - name: Create admin-user ServiceAccount and ClusterRoleBinding (step 22)
    copy:
      dest: /tmp/dashboard-adminuser.yaml
      mode: '0644'
      content: |
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: admin-user
          namespace: {{ dashboard_namespace }}
        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          name: admin-user
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: cluster-admin
        subjects:
          - kind: ServiceAccount
            name: admin-user
            namespace: {{ dashboard_namespace }}

  - name: Apply admin-user access (step 22)
    kubernetes.core.k8s:
      state: present
      src: /tmp/dashboard-adminuser.yaml
      kubeconfig: "{{ kubeconfig }}"

  - name: Create Ingress for Dashboard (step 22)
    copy:
      dest: /tmp/dashboard-ingress.yaml
      mode: '0644'
      content: |
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        metadata:
          name: kubernetes-dashboard
          namespace: {{ dashboard_namespace }}
          annotations:
            nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
            nginx.ingress.kubernetes.io/ssl-redirect: "true"
            nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
        spec:
          ingressClassName: nginx
          tls:
            - hosts:
                - {{ ingress_host }}
          rules:
            - host: {{ ingress_host }}
              http:
                paths:
                  - path: /
                    pathType: Prefix
                    backend:
                      service:
                        name: kubernetes-dashboard-kong-proxy
                        port:
                          number: 443

  - name: Apply Dashboard Ingress (step 22)
    kubernetes.core.k8s:
      state: present
      src: /tmp/dashboard-ingress.yaml
      kubeconfig: "{{ kubeconfig }}"

  - name: Wait for Dashboard proxy service to become available (step 22)
    shell: |
      kubectl wait --namespace={{ dashboard_namespace }} \
        --for=condition=ready pod \
        -l app.kubernetes.io/instance={{ dashboard_release }} \
        --timeout=120s --kubeconfig={{ kubeconfig }}
    changed_when: false

  - name: Display dashboard instructions (step 22)
    debug:
      msg:
        - "Kubernetes Dashboard deployed and accessible via Ingress."
        - ""
        - "To access it, follow these steps:"
        - ""
        - "1. Open your terminal on the host machine and run:"
        - "  sudo nano /etc/hosts"
        - ""
        - "2. Add the following line to the bottom of the file:"
        - "  {{ ingress_ip }}   dashboard.local"
        - ""
        - "3. Save and exit nano:"
        - "  Press Ctrl + O (to write the file), then Enter, then Ctrl + X (to exit)"
        - ""
        - "4. Open your browser and visit:"
        - "  https://dashboard.local"
        - ""
        - "5. To log in, run the following command on the controller VM to generate a token:"
        - "  vagrant ssh ctrl"
        - "  kubectl -n {{ dashboard_namespace }} create token admin-user"
        - ""
        - "6. Paste the token into the Dashboard login screen."

  # # step 23 - install Istio service mesh with static gateway IP
  # - name: Download Istio 1.25.2
  #   get_url:
  #     url: https://github.com/istio/istio/releases/download/1.25.2/istio-1.25.2-linux-amd64.tar.gz
  #     dest: /tmp/istio.tar.gz
  #     mode: '0644'

  # - name: Extract Istio
  #   unarchive:
  #     src: /tmp/istio.tar.gz
  #     dest: /opt/
  #     remote_src: yes

  # - name: Copy istioctl to /usr/local/bin
  #   copy:
  #     src: /opt/istio-1.25.2/bin/istioctl
  #     dest: /usr/local/bin/istioctl
  #     mode: '0755'
  #     remote_src: yes

  # - name: Add istioctl to vagrant user's PATH
  #   lineinfile:
  #     path: /home/vagrant/.bashrc
  #     line: 'export PATH=$PATH:/opt/istio-1.25.2/bin'
  #     state: present

  # - name: Create IstioOperator manifest with static gateway IP
  #   copy:
  #     dest: /tmp/istio-operator.yaml
  #     mode: '0644'
  #     content: |
  #       apiVersion: install.istio.io/v1alpha1
  #       kind: IstioOperator
  #       metadata:
  #         name: istio-install
  #         namespace: istio-system
  #       spec:
  #         profile: default
  #         components:
  #           ingressGateways:
  #             - name: istio-ingressgateway
  #               enabled: true
  #               k8s:
  #                 service:
  #                   type: LoadBalancer
  #                   loadBalancerIP: {{ istio_ip }}
  #         values:
  #           global:
  #             istioNamespace: istio-system

  # - name: Install Istio using IstioOperator manifest
  #   shell: istioctl install -y -f /tmp/istio-operator.yaml --kubeconfig={{ kubeconfig }}
  #   args:
  #     executable: /bin/bash
  #   environment:
  #     PATH: "/usr/local/bin:/usr/bin:/bin"

  # - name: Wait for Istio ingress gateway pod to become ready
  #   shell: |
  #     kubectl wait --namespace=istio-system \
  #       --for=condition=ready pod \
  #       -l app=istio-ingressgateway \
  #       --timeout=120s --kubeconfig={{ kubeconfig }}
  #   changed_when: false


  # --- Assignment 3 ---

  # --------- 1.2: Monitoring ---------
  - name: Add Prometheus Helm repo (Assignment 3 1.2)
    kubernetes.core.helm_repository:
      name: prometheus-community
      repo_url: https://prometheus-community.github.io/helm-charts
      kubeconfig: "{{ kubeconfig }}"

  - name: Install Prometheus Stack via Helm (Assignment 3 1.2)
    kubernetes.core.helm:
      name: "{{ prometheus_release_name }}"
      chart_ref: prometheus-community/kube-prometheus-stack
      update_repo_cache: true
      create_namespace: true
      release_namespace: "{{ monitoring_namespace }}"
      kubeconfig: "{{ kubeconfig }}"
      set_values:
        - value: prometheusOperator.admissionWebhooks.enabled=false
  
  - name: Add ServiceMonitor for application monitoring (Assignment 3 1.2)
    kubernetes.core.k8s:
      state: present
      kubeconfig: "{{ kubeconfig }}"
      definition:
        apiVersion: monitoring.coreos.com/v1
        kind: ServiceMonitor
        metadata:
          name: app-monitor
          namespace: "{{ monitoring_namespace }}"
          labels:
            release: "{{ prometheus_release_name }}"
        spec:
          selector:
            matchLabels:
              app: "{{ app_service_name }}"
          endpoints:
          - interval: 1s  # Should be made larger later
  
  # --------- 1.2: Alerting ---------
  # critical alerts -> on-call team or sysadmins
  # warning alerts -> development team
  # info alerts -> operations team
  # default -> development team
  - name: Configure AlertManager (Assignment 3 1.2)
    kubernetes.core.k8s:
      state: present
      kubeconfig: "{{ kubeconfig }}"
      definition:
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: alertmanager
          namespace: "{{ monitoring_namespace }}"
        data:
          alertmanager.yaml: |
            global:
              resolve_timeout: 5m
              smtp_smarthost: "smtp.google.com:587"
              smtp_from: "{{ .Values.smtp_from }}"
              smtp_auth_username: "{{ .Values.smtp_auth_username }}"
              smtp_auth_password: "{{ .Values.smtp_auth_password }}"
              smtp_require_tls: true
            
            route:
              group_by: ['alertname', 'severity', 'instance']
              group_wait: 30s
              group_interval: 5m
              repeat_interval: 3h
              routes:
              - match:
                  severity: critical
                receiver: 'critical-email'
                continue: true
              - match:
                  severity: warning
                receiver: 'warning-email'
                continue: true
              - match:
                  severity: info
                receiver: 'info-email'
                continue: true
              - receiver: 'warning-email'
            
            receivers:
            - name: "critical-email" 
              email_configs:
              - to: "{{ .Values.critical_email }}"
                send_resolved: true
                headers:
                  subject: "[CRITICAL] {{ template \"email.subject\" . }}"
                html: "{{ template \"email.html\" . }}"
            
            - name: "warning-email"
              email_configs:
              - to: "{{ .Values.warning_email }}"
                send_resolved: true
                headers:
                  subject: "[WARNING] {{ template \"email.subject\" . }}"
                html: "{{ template \"email.html\" . }}"

            - name: "info-email"
              email_configs:
              - to: "{{ .Values.info_email }}"
                send_resolved: true
                headers:
                  subject: "[INFO] {{ template \"email.subject\" . }}"
                html: "{{ template \"email.html\" . }}"

  - name: Add PrometheusRule for alerts (Assignment 3 1.2)
    kubernetes.core.k8s:
      state: present
      kubeconfig: "{{ kubeconfig }}"
      definition:
        apiVersion: monitoring.coreos.com/v1
        kind: PrometheusRule
        metadata:
          name: app-alerts
          labels:
            release: "{{ prometheus_release_name }}"
        spec:
          groups:
          - name: request-rate
            rules:
            - alert: HighRequestRate
              expr: sum(rate(http_requests_total[1m])) > 15
              for: 2m
              labels:
                severity: critical
              annotations:
                summary: "High request rate detected!"
                description: "The application is receiving more than 15 requests per minute for 2 minutes."
            
            - alert: ErrorRateHigh
              expr: sum(rate(http_requests_errors_total[5m])) / sum(rate(http_requests_total[5m])) * 100 > 5
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: "High error rate detected!"
                description: "Error rate is above 5% for the last 5 minutes."

          - name: resource-usage
            rules:
            - alert: HighMemoryUsage
              expr: container_memory_usage_bytes{container!=""} / container_spec_memory_limit_bytes{container!=""} * 100 > 85
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "High memory usage detected!"
                description: "Container {{ $labels.container }} is using more than 85% of its memory limit."

            - alert: HighCPUUsage
              expr: sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (container) * 100 > 80
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "High CPU usage detected!"
                description: "Container {{ $labels.container }} is using more than 80% of CPU."

          - name: availability
            rules:
            - alert: ServiceDown
              expr: up == 0
              for: 1m
              labels:
                severity: critical
              annotations:
                summary: "Service is down!"
                description: "Service {{ $labels.service }} has been down for more than 1 minute."

            - alert: HighLatency
              expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) > 1
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "High latency detected!"
                description: "95th percentile of request latency is above 1 second."
