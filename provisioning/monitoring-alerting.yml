# --- Assignment 3 ---
- name: Monitoring-Alerting
  hosts: all
  become: true
  vars_files:
    - vars/all.yml
  tasks:
  
  # --------- 1.2: Monitoring ---------
  - name: Create monitoring namespace
    kubernetes.core.k8s:
      state: present
      definition:
        apiVersion: v1
        kind: Namespace
        metadata:
          name: "{{ monitoring_namespace }}"
      kubeconfig: "{{ kubeconfig }}"

  - name: Add Prometheus Helm repo
    kubernetes.core.helm_repository:
      name: prometheus-community
      repo_url: https://prometheus-community.github.io/helm-charts
        
  - name: Install Prometheus Stack
    kubernetes.core.helm:
      name: prometheus
      chart_ref: prometheus-community/kube-prometheus-stack
      release_namespace: "{{ monitoring_namespace }}"
      create_namespace: false
      kubeconfig: "{{ kubeconfig }}"
      wait: true
      values:
        prometheus:
          prometheusSpec:
            # must be â‰¥ 60
            maximumStartupDurationSeconds: 120

  - name: Add ServiceMonitor for application monitoring (Assignment 3 1.2)
    kubernetes.core.k8s:
      state: present
      kubeconfig: "{{ kubeconfig }}"
      definition:
        apiVersion: monitoring.coreos.com/v1
        kind: ServiceMonitor
        metadata:
          name: app-monitor
          namespace: "{{ monitoring_namespace }}"
          labels:
            release: prometheus
        spec:
          selector:
            matchLabels:
              app: "{{ app_service_name }}"
          endpoints:
            - port: http
              interval: 20s  # Should be made larger later
  
  # # --------- 1.2: Alerting ---------
  # # critical alerts -> on-call team or sysadmins
  # # warning alerts -> development team
  # # info alerts -> operations team
  # # default -> development team
  # - name: Configure AlertManager (Assignment 3 1.2)
  #   kubernetes.core.k8s:
  #     state: present
  #     kubeconfig: "{{ kubeconfig }}"
  #     definition:
  #       apiVersion: v1
  #       kind: ConfigMap
  #       metadata:
  #         name: alertmanager
  #         namespace: "{{ monitoring_namespace }}"
  #       data:
  #         alertmanager.yaml: |
  #           global:
  #             resolve_timeout: 5m
  #             smtp_smarthost: "smtp.google.com:587"
  #             smtp_from: "{{ .Values.smtp_from }}"
  #             smtp_auth_username: "{{ .Values.smtp_auth_username }}"
  #             smtp_auth_password: "{{ .Values.smtp_auth_password }}"
  #             smtp_require_tls: true
            
  #           route:
  #             group_by: ['alertname', 'severity', 'instance']
  #             group_wait: 30s
  #             group_interval: 5m
  #             repeat_interval: 3h
  #             routes:
  #             - match:
  #                 severity: critical
  #               receiver: 'critical-email'
  #               continue: true
  #             - match:
  #                 severity: warning
  #               receiver: 'warning-email'
  #               continue: true
  #             - match:
  #                 severity: info
  #               receiver: 'info-email'
  #               continue: true
  #             - receiver: 'warning-email'
            
  #           receivers:
  #           - name: "critical-email" 
  #             email_configs:
  #             - to: "{{ .Values.critical_email }}"
  #               send_resolved: true
  #               headers:
  #                 subject: "[CRITICAL] {{ template \"email.subject\" . }}"
  #               html: "{{ template \"email.html\" . }}"
            
  #           - name: "warning-email"
  #             email_configs:
  #             - to: "{{ .Values.warning_email }}"
  #               send_resolved: true
  #               headers:
  #                 subject: "[WARNING] {{ template \"email.subject\" . }}"
  #               html: "{{ template \"email.html\" . }}"

  #           - name: "info-email"
  #             email_configs:
  #             - to: "{{ .Values.info_email }}"
  #               send_resolved: true
  #               headers:
  #                 subject: "[INFO] {{ template \"email.subject\" . }}"
  #               html: "{{ template \"email.html\" . }}"

  # - name: Add PrometheusRule for alerts (Assignment 3 1.2)
  #   kubernetes.core.k8s:
  #     state: present
  #     kubeconfig: "{{ kubeconfig }}"
  #     definition:
  #       apiVersion: monitoring.coreos.com/v1
  #       kind: PrometheusRule
  #       metadata:
  #         name: app-alerts
  #         labels:
  #           release: "{{ prometheus_release_name }}"
  #       spec:
  #         groups:
  #         - name: request-rate
  #           rules:
  #           - alert: HighRequestRate
  #             expr: sum(rate(http_requests_total[1m])) > 15
  #             for: 2m
  #             labels:
  #               severity: critical
  #             annotations:
  #               summary: "High request rate detected!"
  #               description: "The application is receiving more than 15 requests per minute for 2 minutes."
            
  #           - alert: ErrorRateHigh
  #             expr: sum(rate(http_requests_errors_total[5m])) / sum(rate(http_requests_total[5m])) * 100 > 5
  #             for: 5m
  #             labels:
  #               severity: critical
  #             annotations:
  #               summary: "High error rate detected!"
  #               description: "Error rate is above 5% for the last 5 minutes."

  #         - name: resource-usage
  #           rules:
  #           - alert: HighMemoryUsage
  #             expr: container_memory_usage_bytes{container!=""} / container_spec_memory_limit_bytes{container!=""} * 100 > 85
  #             for: 5m
  #             labels:
  #               severity: warning
  #             annotations:
  #               summary: "High memory usage detected!"
  #               description: "Container {{ $labels.container }} is using more than 85% of its memory limit."

  #           - alert: HighCPUUsage
  #             expr: sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (container) * 100 > 80
  #             for: 5m
  #             labels:
  #               severity: warning
  #             annotations:
  #               summary: "High CPU usage detected!"
  #               description: "Container {{ $labels.container }} is using more than 80% of CPU."

  #         - name: availability
  #           rules:
  #           - alert: ServiceDown
  #             expr: up == 0
  #             for: 1m
  #             labels:
  #               severity: critical
  #             annotations:
  #               summary: "Service is down!"
  #               description: "Service {{ $labels.service }} has been down for more than 1 minute."

  #           - alert: HighLatency
  #             expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) > 1
  #             for: 5m
  #             labels:
  #               severity: warning
  #             annotations:
  #               summary: "High latency detected!"
  #               description: "95th percentile of request latency is above 1 second."
